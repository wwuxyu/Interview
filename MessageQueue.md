## Zookeeper对于Kafka的作用是什么？

- Zookeeper 是一个开放源码的、高性能的协调服务，它用于 Kafka 的分布式应用。
- Zookeeper 主要用于在集群中不同节点之间进行通信

在 Kafka 中，它被用于提交偏移量，因此如果节点在任何情况下都失败了，它都 可以从之前提交的偏移量中获取

除此之外，它还执行其他活动，如: leader 检测、分布式同步、配置管理、识别新 节点何时离开或连接、集群、节点实时状态等等



### Kafka数据传输事务定义有哪三种？

和 MQTT 的事务定义一样都是 3 种。

1. 最多一次: 消息不会被重复发送，最多被传输一次，但也有可能一次不传输
2. 最少一次: 消息不会被漏发送，最少被传输一次，但也有可能被重复传输.
3. 精确的一次(Exactly once): 不会漏传输也不会重复传输,每个消息都传输 被一次而且仅仅被传输一次，这是大家所期望的



### Kafka 判断一个节点是否还活着有那两个条件?

1. 节点必须可以维护和 ZooKeeper的连接，Zookeeper通过心跳机制检查每个节点的连接
2. 如果节点是个 follower,他必须能及时的同步leader 的写操作，延时不能太久





### kafka 分布式(不是单机)的情况下，如何保证消息的顺序消费?

Kafka 分布式的单位是 partition，同一个 partition 用一个 write ahead log 组织， 所以可以保证 FIFO 的顺序。不同 partition 之间不能保证顺序。但是绝大多数用 户都可以通过 message key 来定义，因为同一个 key 的 message 可以保证只发 送到同一个 partition。

Kafka 中发送 1 条消息的时候，可以指定(topic, partition, key) 3 个参数。 partiton 和 key 是可选的。如果你指定了 partition，那就是所有消息发往同 1 个 partition，就是有序的。并且在消费端，Kafka 保证，1 个 partition 只能被 1 个 consumer 消费。或者你指定 key(比如 order id)，具有同 1 个 key 的 所有消息，会发往同 1 个 partition。



### kafka 如何不消费重复数据?比如扣款，我们不能重复的扣。

**其实还是得结合业务来思考，我这里给几个思路:**

比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入 了，update 一下好吧。
 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据 的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费 到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗?如果没有消 费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保 证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束 了，重复数据插入只会报错，不会导致数据库中出现脏数据。







### kafka 的高可用机制是什么?

这个问题比较系统，回答出 kafka 的系统特点，leader 和 follower 的关系，消息 读写的顺序即可。

https://www.cnblogs.com/qingyunzong/p/9004703.html











